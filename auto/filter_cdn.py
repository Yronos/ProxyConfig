#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä» domainset æ ¼å¼çš„è§„åˆ™æ–‡ä»¶ä¸­ç­›é€‰å‡ºè¢« rule-set æ ¼å¼è§„åˆ™è¦†ç›–çš„è§„åˆ™
è¾“å‡ºä¸º rule-set æ ¼å¼ï¼ˆ.listï¼‰
"""

# ============================================================
# ğŸ“ é…ç½®åŒºåŸŸ - åœ¨è¿™é‡Œå¡«å…¥ä½ çš„æ–‡ä»¶é“¾æ¥
# ============================================================

# å‰è€…æ–‡ä»¶ (domainset æ ¼å¼) - å¡«å…¥ä¸€ä¸ª URL æˆ–æœ¬åœ°è·¯å¾„
DOMAINSET_URL = "https://ruleset.skk.moe/List/domainset/cdn.conf"

# åè€…æ–‡ä»¶åˆ—è¡¨ (rule-set æ ¼å¼) - å¯ä»¥å¡«å…¥å¤šä¸ª URL æˆ–æœ¬åœ°è·¯å¾„
RULESET_URLS = [
    "https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/refs/heads/master/rule/Surge/GitHub/GitHub.list",
    "https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/refs/heads/master/rule/Surge/Microsoft/Microsoft.list",
    "https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/refs/heads/master/rule/Surge/Google/Google.list",
    "https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/refs/heads/master/rule/Surge/YouTube/YouTube.list",
    "https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/refs/heads/master/rule/Surge/Apple/Apple_All.list",
    "https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/refs/heads/master/rule/Surge/Twitter/Twitter.list",
    "https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/refs/heads/master/rule/Surge/Discord/Discord.list",
    "https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/refs/heads/master/rule/Surge/Reddit/Reddit.list",
    "https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/refs/heads/master/rule/Surge/Cloudflare/Cloudflare.list",
    "https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/refs/heads/master/rule/Surge/Facebook/Facebook.list",
    "https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/refs/heads/master/rule/Surge/Amazon/Amazon.list",
    "https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/refs/heads/master/rule/Surge/Akamai/Akamai.list",
    "https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/refs/heads/master/rule/Surge/Steam/Steam.list",
    "https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/refs/heads/master/rule/Surge/Epic/Epic.list",
    "https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/refs/heads/master/rule/Surge/OpenAI/OpenAI.list",
]

# è‡ªå®šä¹‰è§„åˆ™ - æ‰‹åŠ¨æ·»åŠ æ²¡æœ‰è§„åˆ™é›†çš„æœåŠ¡
# æ ¼å¼ï¼šæ¯è¡Œä¸€æ¡è§„åˆ™ï¼Œæ”¯æŒä¸‰ç§ç±»å‹
CUSTOM_RULES = """
DOMAIN-SUFFIX,fastly.net
DOMAIN-SUFFIX,lencr.org
DOMAIN-SUFFIX,teo-rum.com
DOMAIN-SUFFIX,jsdelivr.net
DOMAIN-SUFFIX,imgur.com
DOMAIN-SUFFIX,stripe.com
DOMAIN-SUFFIX,v2ex.co
DOMAIN-SUFFIX,v2ex.com
DOMAIN-SUFFIX,bsky.social
DOMAIN-SUFFIX,bsky.app
DOMAIN-SUFFIX,wikimedia.org
DOMAIN-SUFFIX,gravatar.com
DOMAIN-SUFFIX,esm.sh
DOMAIN-SUFFIX,111666.best
DOMAIN-SUFFIX,adtidy.org
"""

# è¾“å‡ºæ–‡ä»¶å (ç•™ç©ºåˆ™è‡ªåŠ¨ç”Ÿæˆ)
OUTPUT_FILE = "CDN_Lite.list"

# æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†åŒ¹é…ä¿¡æ¯
VERBOSE = False

# ============================================================
# ä»¥ä¸‹æ˜¯è„šæœ¬ä»£ç ï¼Œæ— éœ€ä¿®æ”¹
# ============================================================

import os
import sys
import time
from typing import Dict, List, Optional, Set, Tuple
from urllib.parse import urlparse

import requests


class SuffixTrie:
    """åå‘å­—å…¸æ ‘ï¼Œç”¨äºé«˜æ•ˆçš„åç¼€åŒ¹é…"""

    def __init__(self):
        self.root: Dict[str, any] = {}

    def add(self, domain: str):
        """æ·»åŠ åç¼€è§„åˆ™ï¼ˆåå‘å­˜å‚¨ï¼‰"""
        parts = domain.lower().split(".")[::-1]
        node = self.root

        for part in parts:
            if part not in node:
                node[part] = {}
            node = node[part]

        node["$"] = True

    def match(self, domain: str) -> bool:
        """æ£€æŸ¥åŸŸåæ˜¯å¦è¢«ä»»ä¸€åç¼€è§„åˆ™è¦†ç›–"""
        parts = domain.lower().split(".")[::-1]
        node = self.root

        for part in parts:
            if part not in node:
                return False
            node = node[part]
            if "$" in node:
                return True

        return False


def convert_github_url(url: str) -> str:
    """å°† GitHub é¡µé¢ URL è½¬æ¢ä¸º raw URL"""
    if "github.com" in url and "/blob/" in url:
        url = url.replace("github.com", "raw.githubusercontent.com")
        url = url.replace("/blob/", "/")
    return url


def download_file(url: str, timeout: int = 30) -> Optional[str]:
    """ä¸‹è½½æ–‡ä»¶å†…å®¹"""
    url = convert_github_url(url)
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        response = requests.get(url, headers=headers, timeout=timeout)
        response.raise_for_status()
        return response.text
    except requests.RequestException as e:
        print(f"[é”™è¯¯] ä¸‹è½½å¤±è´¥ {url}: {e}")
        return None


def read_local_file(filepath: str) -> Optional[str]:
    """è¯»å–æœ¬åœ°æ–‡ä»¶"""
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            return f.read()
    except IOError as e:
        print(f"[é”™è¯¯] è¯»å–æ–‡ä»¶å¤±è´¥ {filepath}: {e}")
        return None


def get_content(path: str) -> Optional[str]:
    """è·å–å†…å®¹ï¼Œæ”¯æŒ URL å’Œæœ¬åœ°æ–‡ä»¶"""
    if path.startswith("http://") or path.startswith("https://"):
        return download_file(path)
    else:
        return read_local_file(path)


def parse_domainset(content: str) -> Tuple[List[str], List[str]]:
    """
    è§£æ domainset æ ¼å¼çš„è§„åˆ™
    è¿”å›: (ç²¾ç¡®åŒ¹é…è§„åˆ™åˆ—è¡¨, åç¼€åŒ¹é…è§„åˆ™åˆ—è¡¨)
    """
    exact_rules = []
    suffix_rules = []

    for line in content.strip().split("\n"):
        line = line.strip()
        if not line or line.startswith("#") or line.startswith("//"):
            continue

        if line.startswith("."):
            suffix_rules.append(line[1:])
        else:
            exact_rules.append(line)

    return exact_rules, suffix_rules


def parse_custom_rules(custom_rules_text: str) -> Tuple[Set[str], Set[str], Set[str]]:
    """
    è§£æè‡ªå®šä¹‰è§„åˆ™
    è¿”å›: (DOMAINé›†åˆ, DOMAIN-SUFFIXé›†åˆ, DOMAIN-KEYWORDé›†åˆ)
    """
    domains = set()
    domain_suffixes = set()
    domain_keywords = set()

    for line in custom_rules_text.strip().split("\n"):
        line = line.strip()
        if not line or line.startswith("#") or line.startswith("//"):
            continue

        parts = line.split(",")
        if len(parts) < 2:
            continue

        rule_type = parts[0].strip().upper()
        value = parts[1].strip().lower()

        if rule_type == "DOMAIN":
            domains.add(value)
        elif rule_type == "DOMAIN-SUFFIX":
            domain_suffixes.add(value)
        elif rule_type == "DOMAIN-KEYWORD":
            domain_keywords.add(value)

    return domains, domain_suffixes, domain_keywords


def parse_ruleset(content: str) -> Tuple[Set[str], Set[str], Set[str]]:
    """
    è§£æ rule-set æ ¼å¼çš„è§„åˆ™
    è¿”å›: (DOMAINé›†åˆ, DOMAIN-SUFFIXé›†åˆ, DOMAIN-KEYWORDé›†åˆ)
    """
    domains = set()
    domain_suffixes = set()
    domain_keywords = set()

    for line in content.strip().split("\n"):
        line = line.strip()
        if not line or line.startswith("#") or line.startswith("//"):
            continue

        parts = line.split(",")
        if len(parts) < 2:
            continue

        rule_type = parts[0].strip().upper()
        value = parts[1].strip().lower()

        if rule_type == "DOMAIN":
            domains.add(value)
        elif rule_type == "DOMAIN-SUFFIX":
            domain_suffixes.add(value)
        elif rule_type == "DOMAIN-KEYWORD":
            domain_keywords.add(value)

    return domains, domain_suffixes, domain_keywords


def build_suffix_matcher(domain_suffixes: Set[str]) -> SuffixTrie:
    """æ„å»ºåç¼€åŒ¹é…å™¨"""
    trie = SuffixTrie()
    for suffix in domain_suffixes:
        trie.add(suffix)
    return trie


def check_keyword_match(domain: str, keywords: Set[str]) -> bool:
    """æ£€æŸ¥åŸŸåæ˜¯å¦åŒ…å«ä»»ä¸€å…³é”®è¯"""
    domain_lower = domain.lower()
    for keyword in keywords:
        if keyword in domain_lower:
            return True
    return False


def check_coverage_batch(
    exact_rules: List[str],
    suffix_rules: List[str],
    domains: Set[str],
    suffix_matcher: SuffixTrie,
    keywords: Set[str],
) -> Tuple[List[Tuple[str, str]], List[Tuple[str, str]]]:
    """
    æ‰¹é‡æ£€æŸ¥è§„åˆ™è¦†ç›–æƒ…å†µ
    è¿”å›: (è¢«è¦†ç›–çš„ç²¾ç¡®è§„åˆ™åˆ—è¡¨, è¢«è¦†ç›–çš„åç¼€è§„åˆ™åˆ—è¡¨)
    æ¯ä¸ªå…ƒç´ ä¸º (åŸå§‹è§„åˆ™, è§„åˆ™ç±»å‹) å…ƒç»„
    """
    covered_exact = []
    covered_suffix = []

    # å¤„ç†ç²¾ç¡®åŒ¹é…è§„åˆ™
    for rule in exact_rules:
        rule_lower = rule.lower()

        # ä¼˜å…ˆçº§ï¼šDOMAIN > DOMAIN-SUFFIX > DOMAIN-KEYWORD
        if rule_lower in domains:
            covered_exact.append((rule, "DOMAIN"))
        elif suffix_matcher.match(rule_lower):
            covered_exact.append((rule, "DOMAIN"))
        elif check_keyword_match(rule_lower, keywords):
            covered_exact.append((rule, "DOMAIN"))

    # å¤„ç†åç¼€åŒ¹é…è§„åˆ™
    for rule in suffix_rules:
        rule_lower = rule.lower()

        if suffix_matcher.match(rule_lower):
            covered_suffix.append((rule, "DOMAIN-SUFFIX"))
        elif check_keyword_match(rule_lower, keywords):
            covered_suffix.append((rule, "DOMAIN-SUFFIX"))

    return covered_exact, covered_suffix


def get_filename_from_path(path: str) -> str:
    """ä»è·¯å¾„æˆ– URL ä¸­æå–æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰"""
    if path.startswith("http://") or path.startswith("https://"):
        path = urlparse(path).path
    filename = os.path.basename(path)
    name, _ = os.path.splitext(filename)
    return name


def generate_output_filename(ruleset_names: List[str], domainset_name: str) -> str:
    """ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å"""
    if not ruleset_names:
        ruleset_names = ["unknown"]
    return "-".join(ruleset_names) + "-" + domainset_name + ".list"


def ensure_output_dir():
    """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_dir = os.path.join(os.path.dirname(script_dir), "rules")

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"[ä¿¡æ¯] åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")

    return output_dir


def main():
    print("=" * 60)
    print("è§„åˆ™åˆå¹¶ä¸æå–è„šæœ¬")
    print("=" * 60)

    start_time = time.time()

    # è·å–å¹¶è§£æå‰è€…æ–‡ä»¶ï¼ˆdomainsetï¼‰
    print(f"\n[1] è·å–å‰è€…æ–‡ä»¶ (domainset):")
    print(f"    {DOMAINSET_URL}")
    domainset_content = get_content(DOMAINSET_URL)
    if domainset_content is None:
        print("[é”™è¯¯] æ— æ³•è·å–å‰è€…æ–‡ä»¶ï¼Œé€€å‡º")
        sys.exit(1)

    exact_rules, suffix_rules = parse_domainset(domainset_content)
    total_rules = len(exact_rules) + len(suffix_rules)
    print(f"    è§£æåˆ° {total_rules} æ¡è§„åˆ™")
    print(f"      - ç²¾ç¡®åŒ¹é…: {len(exact_rules)} æ¡")
    print(f"      - åç¼€åŒ¹é…: {len(suffix_rules)} æ¡")

    if total_rules == 0:
        print("[é”™è¯¯] å‰è€…æ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆè§„åˆ™ï¼Œé€€å‡º")
        sys.exit(1)

    # è§£æè‡ªå®šä¹‰è§„åˆ™
    print(f"\n[2] è§£æè‡ªå®šä¹‰è§„åˆ™:")
    custom_domains, custom_suffixes, custom_keywords = parse_custom_rules(CUSTOM_RULES)
    custom_total = len(custom_domains) + len(custom_suffixes) + len(custom_keywords)
    print(f"    è§£æåˆ° {custom_total} æ¡è‡ªå®šä¹‰è§„åˆ™")
    print(f"      - DOMAIN: {len(custom_domains)} æ¡")
    print(f"      - DOMAIN-SUFFIX: {len(custom_suffixes)} æ¡")
    print(f"      - DOMAIN-KEYWORD: {len(custom_keywords)} æ¡")

    # è·å–å¹¶è§£æåè€…æ–‡ä»¶ï¼ˆrule-setï¼‰
    print(f"\n[3] è·å–åè€…æ–‡ä»¶ (rule-set):")
    all_domains: Set[str] = set(custom_domains)
    all_domain_suffixes: Set[str] = set(custom_suffixes)
    all_domain_keywords: Set[str] = set(custom_keywords)
    ruleset_names: List[str] = []

    for ruleset_path in RULESET_URLS:
        print(f"    - {ruleset_path}")
        content = get_content(ruleset_path)
        if content is None:
            print(f"      [è­¦å‘Š] è·³è¿‡æ­¤æ–‡ä»¶")
            continue

        domains, domain_suffixes, domain_keywords = parse_ruleset(content)
        all_domains.update(domains)
        all_domain_suffixes.update(domain_suffixes)
        all_domain_keywords.update(domain_keywords)
        ruleset_names.append(get_filename_from_path(ruleset_path))
        print(
            f"      DOMAIN: {len(domains)}, DOMAIN-SUFFIX: {len(domain_suffixes)}, DOMAIN-KEYWORD: {len(domain_keywords)}"
        )

    if not all_domains and not all_domain_suffixes and not all_domain_keywords:
        print("[é”™è¯¯] åè€…æ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆè§„åˆ™ï¼Œé€€å‡º")
        sys.exit(1)

    print(f"\n[4] åè€…è§„åˆ™æ±‡æ€»:")
    print(f"    DOMAIN è§„åˆ™æ€»æ•°: {len(all_domains)} (å«è‡ªå®šä¹‰ {len(custom_domains)})")
    print(
        f"    DOMAIN-SUFFIX è§„åˆ™æ€»æ•°: {len(all_domain_suffixes)} (å«è‡ªå®šä¹‰ {len(custom_suffixes)})"
    )
    print(
        f"    DOMAIN-KEYWORD è§„åˆ™æ€»æ•°: {len(all_domain_keywords)} (å«è‡ªå®šä¹‰ {len(custom_keywords)})"
    )

    # æ„å»ºåç¼€åŒ¹é…å™¨
    print(f"\n[5] æ„å»ºåç¼€åŒ¹é…å™¨...")
    suffix_matcher = build_suffix_matcher(all_domain_suffixes)
    print(f"    å®Œæˆ")

    # æ‰¹é‡æ£€æŸ¥è¦†ç›–æƒ…å†µ
    print(f"\n[6] å¼€å§‹æ‰¹é‡ç­›é€‰...")
    covered_exact, covered_suffix = check_coverage_batch(
        exact_rules, suffix_rules, all_domains, suffix_matcher, all_domain_keywords
    )

    total_covered = len(covered_exact) + len(covered_suffix)
    print(f"    ç­›é€‰å®Œæˆ: {total_covered}/{total_rules} æ¡è§„åˆ™è¢«ä¿ç•™")
    print(f"      - ç²¾ç¡®åŒ¹é…: {len(covered_exact)}/{len(exact_rules)}")
    print(f"      - åç¼€åŒ¹é…: {len(covered_suffix)}/{len(suffix_rules)}")

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = ensure_output_dir()

    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
    if OUTPUT_FILE:
        output_filename = OUTPUT_FILE
    else:
        domainset_name = get_filename_from_path(DOMAINSET_URL)
        output_filename = generate_output_filename(ruleset_names, domainset_name)

    output_path = os.path.join(output_dir, output_filename)

    # å†™å…¥è¾“å‡ºæ–‡ä»¶ï¼ˆrule-set æ ¼å¼ï¼‰
    print(f"\n[7] å†™å…¥è¾“å‡ºæ–‡ä»¶: {output_path}")
    try:
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(f"# è§„åˆ™åˆå¹¶ä¸æå–ç»“æœ\n")
            f.write(f"# å‰è€…æ–‡ä»¶: {DOMAINSET_URL}\n")
            f.write(f"# åè€…æ–‡ä»¶: {', '.join(RULESET_URLS)}\n")
            f.write(f"# è‡ªå®šä¹‰è§„åˆ™æ•°: {custom_total}\n")
            f.write(f"# å‰è€…è§„åˆ™æ€»æ•°: {total_rules}\n")
            f.write(f"# ç­›é€‰åè§„åˆ™æ•°: {total_covered}\n")
            f.write(f"# è¦†ç›–ç‡: {total_covered / total_rules * 100:.2f}%\n")
            f.write(f"#\n")

            # å†™å…¥ç²¾ç¡®åŒ¹é…è§„åˆ™
            for rule, rule_type in covered_exact:
                f.write(f"{rule_type},{rule}\n")

            # å†™å…¥åç¼€åŒ¹é…è§„åˆ™
            for rule, rule_type in covered_suffix:
                f.write(f"{rule_type},{rule}\n")

        print(f"    æˆåŠŸå†™å…¥ {total_covered} æ¡è§„åˆ™")
    except IOError as e:
        print(f"[é”™è¯¯] å†™å…¥æ–‡ä»¶å¤±è´¥: {e}")
        sys.exit(1)

    elapsed_time = time.time() - start_time

    print("\n" + "=" * 60)
    print("å¤„ç†å®Œæˆ!")
    print("=" * 60)

    print(f"\nç»Ÿè®¡ä¿¡æ¯:")
    print(f"  å‰è€…è§„åˆ™æ€»æ•°: {total_rules}")
    print(f"  è‡ªå®šä¹‰è§„åˆ™æ•°: {custom_total}")
    print(f"  ç­›é€‰åè§„åˆ™æ•°: {total_covered}")
    print(f"  è¦†ç›–ç‡: {total_covered / total_rules * 100:.2f}%")
    print(f"  å¤„ç†è€—æ—¶: {elapsed_time:.2f} ç§’")
    print(f"  è¾“å‡ºæ–‡ä»¶: {output_path}")


if __name__ == "__main__":
    main()
