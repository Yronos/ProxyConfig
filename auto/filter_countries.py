import os
import re
from collections import defaultdict
from datetime import datetime
from urllib.parse import urlparse

import requests


class DomainRuleFilter:
    def __init__(self):
        """
        åˆå§‹åŒ–è¿‡æ»¤å™¨
        è„šæœ¬ä½äº ./auto ç›®å½•
        åŸå§‹æ–‡ä»¶å­˜æ”¾åœ¨ ./auto/original
        è¿‡æ»¤åæ–‡ä»¶å­˜æ”¾åœ¨ ./auto/new
        """
        # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
        self.script_dir = os.path.dirname(os.path.abspath(__file__))

        # è®¾ç½®å­ç›®å½•è·¯å¾„
        self.original_dir = os.path.join(self.script_dir, "original")
        self.new_dir = os.path.join(self.script_dir, "new")

        # å¸¸è§å›½å®¶/åœ°åŒºé¡¶çº§åŸŸååç¼€
        self.regional_tlds = {
            # å›½å®¶ä»£ç é¡¶çº§åŸŸå
            "us",
            "uk",
            "cn",
            "jp",
            "kr",
            "de",
            "fr",
            "ca",
            "au",
            "in",
            "br",
            "ru",
            "it",
            "es",
            "nl",
            "se",
            "ch",
            "mx",
            "ar",
            "tw",
            "hk",
            "sg",
            "my",
            "th",
            "id",
            "ph",
            "vn",
            "za",
            "ae",
            "sa",
            "eg",
            "pk",
            "bd",
            "ng",
            "ke",
            "gh",
            "tn",
            "ma",
            "dz",
            "iq",
            "af",
            "ye",
            "sy",
            "jo",
            "lb",
            "kw",
            "om",
            "qa",
            "bh",
            "il",
            "ps",
            "tr",
            "ir",
            "kz",
            "uz",
            "tm",
            "tj",
            "kg",
            "mn",
            "np",
            "lk",
            "mm",
            "kh",
            "la",
            "bn",
            "bt",
            "mv",
            "fj",
            "pg",
            "nc",
            "pf",
            "ck",
            "ws",
            "to",
            "vu",
            "sb",
            "ki",
            "nr",
            "tv",
            "pw",
            "mh",
            "fm",
            "mp",
            "gu",
            "as",
            "vi",
            "pr",
            "do",
            "cu",
            "jm",
            "ht",
            "bs",
            "bb",
            "tt",
            "gy",
            "sr",
            "gf",
            "ve",
            "co",
            "ec",
            "pe",
            "bo",
            "py",
            "uy",
            "cl",
            "cr",
            "pa",
            "ni",
            "hn",
            "sv",
            "gt",
            "bz",
            "cz",
            "sk",
            "pl",
            "hu",
            "ro",
            "bg",
            "hr",
            "si",
            "ba",
            "rs",
            "me",
            "mk",
            "al",
            "gr",
            "cy",
            "mt",
            "is",
            "ie",
            "pt",
            "dk",
            "no",
            "fi",
            "ee",
            "lv",
            "lt",
            "by",
            "ua",
            "md",
            "ge",
            "am",
            "az",
            "at",
            "be",
            "lu",
            "li",
            "mc",
            "ad",
            "sm",
            "va",
            "nz",
            "tl",
            "sc",
            "mu",
            "re",
            "yt",
            "km",
            "mg",
            "mw",
            "zm",
            "zw",
            "bw",
            "na",
            "sz",
            "ls",
            "ao",
            "mz",
            "tz",
            "ug",
            "rw",
            "bi",
            "et",
            "er",
            "dj",
            "so",
            "sd",
            "ss",
            "td",
            "cf",
            "cm",
            "gq",
            "ga",
            "cg",
            "cd",
            "st",
            "gw",
            "gm",
            "sn",
            "mr",
            "ml",
            "bf",
            "ne",
            "ci",
            "tg",
            "bj",
            "lr",
            "sl",
            "cv",
        }

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.original_dir, exist_ok=True)
        os.makedirs(self.new_dir, exist_ok=True)

    def download_rule_list(self, url):
        """ä¸‹è½½è§„åˆ™åˆ—è¡¨åˆ°originalæ–‡ä»¶å¤¹"""
        try:
            response = requests.get(url, timeout=30)
            response.raise_for_status()

            # ä»URLä¸­æå–æ–‡ä»¶å
            filename = os.path.basename(urlparse(url).path)
            if not filename:
                filename = "rules.list"

            filepath = os.path.join(self.original_dir, filename)

            with open(filepath, "w", encoding="utf-8") as f:
                f.write(response.text)

            print(f"  âœ“ å·²ä¸‹è½½: {filename}")
            return filepath, filename

        except Exception as e:
            print(f"  âœ— ä¸‹è½½å¤±è´¥: {e}")
            return None, None

    def parse_header(self, lines):
        """
        è§£ææ–‡ä»¶å¤´éƒ¨ä¿¡æ¯
        è¿”å›: (header_lines, header_info, content_start_index)
        """
        header_lines = []
        header_info = {}
        content_start = 0

        for idx, line in enumerate(lines):
            stripped = line.strip()
            if stripped.startswith("#"):
                header_lines.append(line)

                # è§£æå¤´éƒ¨ä¿¡æ¯
                if ":" in stripped:
                    parts = stripped[1:].split(":", 1)
                    if len(parts) == 2:
                        key = parts[0].strip()
                        value = parts[1].strip()
                        header_info[key] = value
            else:
                # é‡åˆ°ç¬¬ä¸€ä¸ªéæ³¨é‡Šè¡Œï¼Œå¤´éƒ¨ç»“æŸ
                content_start = idx
                break

        return header_lines, header_info, content_start

    def count_rule_types(self, lines, start_index=0):
        """
        ç»Ÿè®¡å„ç±»è§„åˆ™çš„æ•°é‡
        """
        counts = defaultdict(int)

        for line in lines[start_index:]:
            stripped = line.strip()
            if not stripped or stripped.startswith("#"):
                continue

            # è¯†åˆ«è§„åˆ™ç±»å‹
            if stripped.startswith("DOMAIN-SUFFIX,"):
                counts["DOMAIN-SUFFIX"] += 1
            elif stripped.startswith("DOMAIN-KEYWORD,"):
                counts["DOMAIN-KEYWORD"] += 1
            elif stripped.startswith("DOMAIN,"):
                counts["DOMAIN"] += 1
            elif stripped.startswith("IP-CIDR6,"):
                counts["IP-CIDR6"] += 1
            elif stripped.startswith("IP-CIDR,"):
                counts["IP-CIDR"] += 1
            elif stripped.startswith("IP6-CIDR,"):
                counts["IP-CIDR6"] += 1
            elif stripped.startswith("USER-AGENT,"):
                counts["USER-AGENT"] += 1
            elif stripped.startswith("URL-REGEX,"):
                counts["URL-REGEX"] += 1
            elif stripped.startswith("PROCESS-NAME,"):
                counts["PROCESS-NAME"] += 1

        return counts

    def generate_header(self, header_info, rule_counts):
        """
        ç”Ÿæˆæ›´æ–°åçš„æ–‡ä»¶å¤´éƒ¨
        """
        header_lines = []

        # ä¿ç•™åŸæœ‰çš„NAME, AUTHOR, REPO
        if "NAME" in header_info:
            header_lines.append(f"# NAME: {header_info['NAME']}\n")

        # æ›´æ–°æ—¶é—´
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        header_lines.append(f"# UPDATED: {current_time}\n")

        # æ·»åŠ è§„åˆ™ç»Ÿè®¡ï¼ˆæŒ‰å­—æ¯é¡ºåºï¼‰
        total = 0
        for rule_type in sorted(rule_counts.keys()):
            count = rule_counts[rule_type]
            total += count
            header_lines.append(f"# {rule_type}: {count}\n")

        # æ€»è®¡
        header_lines.append(f"# TOTAL: {total}\n")

        return header_lines

    def extract_domain_from_line(self, line):
        """ä»è§„åˆ™è¡Œä¸­æå–åŸŸåå’Œè§„åˆ™ç±»å‹"""
        line = line.strip()

        # è·³è¿‡æ³¨é‡Šå’Œç©ºè¡Œ
        if not line or line.startswith("#"):
            return None, None

        # å¤„ç†ä¸åŒçš„è§„åˆ™æ ¼å¼
        patterns = [
            (r"DOMAIN-SUFFIX,([^,\s]+)", "DOMAIN-SUFFIX"),
            (r"DOMAIN,([^,\s]+)", "DOMAIN"),
            (r"DOMAIN-KEYWORD,([^,\s]+)", "DOMAIN-KEYWORD"),
        ]

        for pattern, rule_type in patterns:
            match = re.search(pattern, line)
            if match:
                return match.group(1).lower(), rule_type

        return None, None

    def get_base_domain(self, domain):
        """
        æå–åŸºç¡€åŸŸåï¼ˆä¸»åŸŸåï¼‰
        ä¾‹å¦‚ï¼š
        youtube.az -> youtube
        youtube.com -> youtube
        ggpht.cn -> ggpht
        www.youtube.com -> youtube
        """
        if not domain:
            return None

        # ç§»é™¤ www. å‰ç¼€
        domain = re.sub(r"^www\.", "", domain)

        # åˆ†å‰²åŸŸå
        parts = domain.split(".")

        if len(parts) < 2:
            return domain

        # å¦‚æœæœ€åä¸€ä¸ªéƒ¨åˆ†æ˜¯åŒºåŸŸæ€§TLDï¼Œè¿”å›å€’æ•°ç¬¬äºŒä¸ªéƒ¨åˆ†
        if parts[-1] in self.regional_tlds:
            return parts[-2] if len(parts) >= 2 else domain

        # å¯¹äºé€šç”¨TLD (com, org, netç­‰)ï¼Œä¹Ÿè¿”å›å€’æ•°ç¬¬äºŒä¸ªéƒ¨åˆ†
        common_tlds = {
            "com",
            "org",
            "net",
            "edu",
            "gov",
            "mil",
            "int",
            "info",
            "biz",
            "io",
        }
        if parts[-1] in common_tlds:
            return parts[-2] if len(parts) >= 2 else domain

        # å…¶ä»–æƒ…å†µè¿”å›å€’æ•°ç¬¬äºŒä¸ªéƒ¨åˆ†
        return parts[-2] if len(parts) >= 2 else domain

    def is_regional_variant(self, domain):
        """
        åˆ¤æ–­åŸŸåæ˜¯å¦æ˜¯åŒºåŸŸæ€§å˜ä½“
        """
        if not domain:
            return False

        parts = domain.split(".")
        if len(parts) < 2:
            return False

        # æ£€æŸ¥TLDæ˜¯å¦æ˜¯å›½å®¶/åœ°åŒºä»£ç 
        tld = parts[-1]
        return tld in self.regional_tlds

    def filter_rules(self, input_file, output_file, threshold=5):
        """è¿‡æ»¤è§„åˆ™æ–‡ä»¶ï¼Œç§»é™¤åŒºåŸŸæ€§åŸŸåå˜ä½“"""
        try:
            with open(input_file, "r", encoding="utf-8") as f:
                lines = f.readlines()

            # è§£ææ–‡ä»¶å¤´éƒ¨
            header_lines, header_info, content_start = self.parse_header(lines)

            # ç¬¬ä¸€éï¼šåˆ†æåŸŸåï¼Œç»Ÿè®¡æ¯ä¸ªåŸºç¡€åŸŸåçš„åŒºåŸŸæ€§å˜ä½“æ•°é‡
            base_domain_variants = defaultdict(list)
            domain_info = {}

            for idx in range(content_start, len(lines)):
                line = lines[idx]
                domain, rule_type = self.extract_domain_from_line(line)
                if domain:
                    base_domain = self.get_base_domain(domain)
                    is_regional = self.is_regional_variant(domain)

                    domain_info[idx] = (domain, rule_type, is_regional)

                    if is_regional and base_domain:
                        base_domain_variants[base_domain].append((domain, idx))

            # æ‰¾å‡ºéœ€è¦è¿‡æ»¤çš„åŸºç¡€åŸŸå
            base_domains_to_filter = set()
            for base_domain, variants in base_domain_variants.items():
                if len(variants) >= threshold:
                    base_domains_to_filter.add(base_domain)

            print(f"\n  ğŸ“‹ åŸŸåå˜ä½“åˆ†æ:")
            for base_domain in sorted(base_domain_variants.keys()):
                count = len(base_domain_variants[base_domain])
                if base_domain in base_domains_to_filter:
                    print(f"    ğŸ—‘ï¸  {base_domain}: {count} ä¸ªåŒºåŸŸå˜ä½“ â†’ å°†å‰”é™¤")
                else:
                    print(f"    âœ… {base_domain}: {count} ä¸ªåŒºåŸŸå˜ä½“ â†’ ä¿ç•™")

                # æ˜¾ç¤ºéƒ¨åˆ†åŸŸåç¤ºä¾‹
                if count <= 10:
                    for domain, _ in base_domain_variants[base_domain][:5]:
                        symbol = (
                            "    â”œâ”€"
                            if base_domain in base_domains_to_filter
                            else "    â”œâ”€"
                        )
                        print(f"{symbol} {domain}")
                    if count > 5:
                        print(f"    â””â”€ ... è¿˜æœ‰ {count - 5} ä¸ª")

            # ç¬¬äºŒéï¼šè¿‡æ»¤è§„åˆ™
            filtered_lines = []
            removed_count = 0
            removed_domains = []

            for idx, line in enumerate(lines):
                # è·³è¿‡å¤´éƒ¨ï¼ˆå¤´éƒ¨ä¼šé‡æ–°ç”Ÿæˆï¼‰
                if idx < content_start:
                    continue

                # ä¿ç•™éåŸŸåè§„åˆ™
                if idx not in domain_info:
                    filtered_lines.append(line)
                    continue

                domain, rule_type, is_regional = domain_info[idx]
                base_domain = self.get_base_domain(domain)

                # è¿‡æ»¤åŒºåŸŸæ€§å˜ä½“
                if is_regional and base_domain in base_domains_to_filter:
                    removed_count += 1
                    removed_domains.append(domain)
                    continue

                filtered_lines.append(line)

            # ç»Ÿè®¡è¿‡æ»¤åçš„è§„åˆ™æ•°é‡
            rule_counts = self.count_rule_types(filtered_lines)

            # ç”Ÿæˆæ–°çš„å¤´éƒ¨
            new_header = self.generate_header(header_info, rule_counts)

            # å†™å…¥è¿‡æ»¤åçš„æ–‡ä»¶
            with open(output_file, "w", encoding="utf-8") as f:
                f.writelines(new_header)
                f.writelines(filtered_lines)

            print(f"\n  ğŸ“Š å¤„ç†ç»Ÿè®¡:")
            print(f"    â€¢ åŸå§‹è§„åˆ™æ•°: {len(lines) - content_start}")
            print(f"    â€¢ è¿‡æ»¤åè§„åˆ™: {len(filtered_lines)}")
            print(f"    â€¢ å·²ç§»é™¤è§„åˆ™: {removed_count}")
            print(
                f"    â€¢ ä¿ç•™æ¯”ä¾‹: {len(filtered_lines) / (len(lines) - content_start) * 100:.1f}%"
            )

            if removed_domains and len(removed_domains) <= 20:
                print(f"\n  ğŸ—‘ï¸  ç§»é™¤çš„åŸŸåç¤ºä¾‹ (å‰ {min(len(removed_domains), 20)} ä¸ª):")
                for i, domain in enumerate(removed_domains[:20], 1):
                    print(f"    {i:2d}. {domain}")
                if len(removed_domains) > 20:
                    print(f"    ... è¿˜æœ‰ {len(removed_domains) - 20} ä¸ªåŸŸåè¢«ç§»é™¤")

            return True

        except Exception as e:
            print(f"  âŒ å¤„ç†å¤±è´¥: {e}")
            import traceback

            traceback.print_exc()
            return False

    def process_url(self, url, threshold=5):
        """å¤„ç†å•ä¸ªURL"""
        print(f"\n{'=' * 70}")
        print(f"ğŸš€ å¼€å§‹å¤„ç†è§„åˆ™åˆ—è¡¨")
        print(f"{'=' * 70}")
        print(f"ğŸ“ URL: {url}")
        print(f"ğŸ¯ é˜ˆå€¼: {threshold} (åŒºåŸŸå˜ä½“æ•°)")

        # ä¸‹è½½
        input_file, filename = self.download_rule_list(url)
        if not input_file:
            return False

        # è¿‡æ»¤
        output_file = os.path.join(self.new_dir, filename)
        success = self.filter_rules(input_file, output_file, threshold)

        if success:
            print(f"\nâœ… å¤„ç†å®Œæˆ!")
            print(f"ğŸ“ åŸå§‹æ–‡ä»¶: {os.path.relpath(input_file)}")
            print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {os.path.relpath(output_file)}")
        else:
            print(f"\nâŒ å¤„ç†å¤±è´¥!")

        return success

    def process_urls(self, urls, threshold=5):
        """æ‰¹é‡å¤„ç†å¤šä¸ªURL"""
        print(f"\n{'=' * 70}")
        print(f"ğŸš€ æ‰¹é‡å¤„ç†æ¨¡å¼")
        print(f"{'=' * 70}")
        print(f"ğŸ“¦ å¾…å¤„ç†åˆ—è¡¨: {len(urls)} ä¸ª")
        print(f"ğŸ¯ é˜ˆå€¼: {threshold}")

        results = []
        for i, url in enumerate(urls, 1):
            print(f"\n[{i}/{len(urls)}] å¤„ç†ä¸­...")
            success = self.process_url(url, threshold)
            results.append((url, success))

        # æ±‡æ€»ç»“æœ
        print(f"\n{'=' * 70}")
        print(f"ğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆæ±‡æ€»")
        print(f"{'=' * 70}")
        success_count = sum(1 for _, success in results if success)
        fail_count = len(results) - success_count

        print(f"âœ… æˆåŠŸ: {success_count} ä¸ª")
        print(f"âŒ å¤±è´¥: {fail_count} ä¸ª")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {success_count / len(results) * 100:.1f}%")

        print(f"\nè¯¦ç»†ç»“æœ:")
        for url, success in results:
            status = "âœ…" if success else "âŒ"
            filename = os.path.basename(urlparse(url).path)
            print(f"  {status} {filename}")

        return results


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("=" * 70)
    print("ğŸ”§ åŸŸåè§„åˆ™è¿‡æ»¤å·¥å…·")
    print("=" * 70)
    print("ğŸ“ åŠŸèƒ½: å‰”é™¤åŒºåŸŸæ€§åŸŸåå˜ä½“ï¼Œä¿ç•™é€šç”¨åŸŸå")
    print("=" * 70)

    # åˆå§‹åŒ–è¿‡æ»¤å™¨ï¼ˆè‡ªåŠ¨ä½¿ç”¨ ./auto/original å’Œ ./auto/newï¼‰
    filter_tool = DomainRuleFilter()

    # å•ä¸ªURLå¤„ç†
    # url = "https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/refs/heads/master/rule/Clash/YouTube/YouTube.list"
    # filter_tool.process_url(url, threshold=5)

    # æ‰¹é‡å¤„ç†å¤šä¸ªURLç¤ºä¾‹
    urls = [
        "https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/refs/heads/master/rule/Clash/YouTube/YouTube.list",
        "https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/refs/heads/master/rule/Clash/Facebook/Facebook.list",
        "https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/refs/heads/master/rule/Clash/Google/Google.list",
    ]
    filter_tool.process_urls(urls, threshold=5)

    print("\n" + "=" * 70)
    print("âœ¨ ç¨‹åºæ‰§è¡Œå®Œæ¯•")
    print("=" * 70)
