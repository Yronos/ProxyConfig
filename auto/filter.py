import os
import re
from collections import defaultdict
from datetime import datetime
from urllib.parse import urlparse

import requests


class DomainRuleFilter:
    def __init__(self):
        """
        åˆå§‹åŒ–è¿‡æ»¤å™¨
        è„šæœ¬ä½äº ./auto ç›®å½•
        åŸå§‹æ–‡ä»¶å­˜æ”¾åœ¨ ./auto/original/{rule_type}
        è¿‡æ»¤åæ–‡ä»¶å­˜æ”¾åœ¨ ./auto/new/{rule_type}
        """
        # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
        self.script_dir = os.path.dirname(os.path.abspath(__file__))

        # è®¾ç½®åŸºç¡€ç›®å½•è·¯å¾„
        self.original_base_dir = os.path.join(self.script_dir, "original")
        self.new_base_dir = os.path.join(self.script_dir, "new")

        # è§„åˆ™ç±»å‹æ˜ å°„è¡¨ï¼ˆURLè·¯å¾„å…³é”®å­— -> ç›®å½•åï¼‰
        self.rule_type_mapping = {
            "clash": "mihomo",
            "mihomo": "mihomo",
            "loon": "loon",
            "surge": "surge",
            "quantumultx": "quantumultx",
            "quantumult": "quantumult",
            "shadowrocket": "shadowrocket",
            "stash": "stash",
            "egern": "egern",
            "singbox": "singbox",
            "sing-box": "singbox",
        }

        # äºŒçº§åŸŸåæ ‡è¯†é›†åˆ
        self.second_level_indicators = {
            "com",
            # "co",
            "net",
            "org",
            "gov",
            "edu",
            "ac",
            "mil",
            "nom",
            "sch",
            "gob",
            "int",
        }

        # ã€é‡è¦ã€‘å…·æœ‰ç‰¹æ®Šæ„ä¹‰çš„ccTLDï¼ˆè™½ç„¶æ˜¯å›½å®¶ä»£ç ï¼Œä½†è¢«å¹¿æ³›ç”¨ä½œé€šç”¨åŸŸåï¼‰
        # è¿™äº›ä¸åº”è¢«è§†ä¸ºåŒºåŸŸå˜ä½“
        self.special_purpose_cctlds = {
            "io",  # ç§‘æŠ€/åˆåˆ›å…¬å¸ï¼ˆè‹±å±å°åº¦æ´‹é¢†åœ°ï¼‰
            "ai",  # AI/äººå·¥æ™ºèƒ½ï¼ˆå®‰åœ­æ‹‰ï¼‰
            "co",  # .comçš„æ›¿ä»£å“ï¼ˆå“¥ä¼¦æ¯”äºšï¼‰
            "gg",  # æ¸¸æˆ/ç¤¾åŒºï¼ˆæ ¹è¥¿å²›ï¼‰
            "tv",  # è§†é¢‘/åª’ä½“ï¼ˆå›¾ç“¦å¢ï¼‰
            "me",  # ä¸ªäººå“ç‰Œï¼ˆé»‘å±±ï¼‰
            "fm",  # éŸ³é¢‘/å¹¿æ’­ï¼ˆå¯†å…‹ç½—å°¼è¥¿äºšï¼‰
            "cc",  # é€šç”¨ç”¨é€”ï¼ˆç§‘ç§‘æ–¯ç¾¤å²›ï¼‰
            "ws",  # ç½‘ç«™æœåŠ¡ï¼ˆè¨æ‘©äºšï¼‰
            "to",  # URLç¼©çŸ­ï¼ˆæ±¤åŠ ï¼‰
            "sh",  # Shell/å¼€å‘ï¼ˆåœ£èµ«å‹’æ‹¿ï¼‰
            "nu",  # é€šç”¨ï¼ˆçº½åŸƒï¼‰
            "tk",  # å…è´¹åŸŸåï¼ˆæ‰˜å…‹åŠ³ï¼‰
        }

        # é€šç”¨é¡¶çº§åŸŸå
        self.generic_tlds = {
            "com",
            "org",
            "net",
            "edu",
            "gov",
            "mil",
            "int",
            "info",
            "biz",
            "app",
            "dev",
            "xyz",
            "online",
            "site",
            "tech",
            "store",
            "club",
            "top",
            "vip",
            "pro",
            "ventures",
            "wiki",
            "ink",
            "link",
            "work",
            "today",
            "world",
            "life",
            "space",
            "solutions",
        }

        # çœŸæ­£çš„åŒºåŸŸæ€§é¡¶çº§åŸŸåï¼ˆæ’é™¤ç‰¹æ®Šç”¨é€”ccTLDï¼‰
        self.regional_tlds = {
            "us",
            "uk",
            "cn",
            "jp",
            "kr",
            "de",
            "fr",
            "ca",
            "au",
            "in",
            "br",
            "ru",
            "it",
            "es",
            "nl",
            "se",
            "ch",
            "mx",
            "ar",
            "tw",
            "hk",
            "sg",
            "my",
            "th",
            "id",
            "ph",
            "vn",
            "za",
            "ae",
            "sa",
            "eg",
            "pk",
            "bd",
            "ng",
            "ke",
            "gh",
            "tn",
            "ma",
            "dz",
            "iq",
            "af",
            "ye",
            "sy",
            "jo",
            "lb",
            "kw",
            "om",
            "qa",
            "bh",
            "il",
            "ps",
            "tr",
            "ir",
            "kz",
            "uz",
            "tm",
            "tj",
            "kg",
            "mn",
            "np",
            "lk",
            "mm",
            "kh",
            "la",
            "bn",
            "bt",
            "mv",
            "fj",
            "pg",
            "nc",
            "pf",
            "ck",
            "vu",
            "sb",
            "ki",
            "nr",
            "pw",
            "mh",
            "mp",
            "gu",
            "as",
            "vi",
            "pr",
            "do",
            "cu",
            "jm",
            "ht",
            "bs",
            "bb",
            "tt",
            "gy",
            "sr",
            "gf",
            "ve",
            "ec",
            "pe",
            "bo",
            "py",
            "uy",
            "cl",
            "cr",
            "pa",
            "ni",
            "hn",
            "sv",
            "gt",
            "bz",
            "cz",
            "sk",
            "pl",
            "hu",
            "ro",
            "bg",
            "hr",
            "si",
            "ba",
            "rs",
            "mk",
            "al",
            "gr",
            "cy",
            "mt",
            "is",
            "ie",
            "pt",
            "dk",
            "no",
            "fi",
            "ee",
            "lv",
            "lt",
            "by",
            "ua",
            "md",
            "ge",
            "am",
            "az",
            "at",
            "be",
            "lu",
            "li",
            "mc",
            "ad",
            "sm",
            "va",
            "nz",
            "tl",
            "sc",
            "mu",
            "re",
            "yt",
            "km",
            "mg",
            "mw",
            "zm",
            "zw",
            "bw",
            "na",
            "sz",
            "ls",
            "ao",
            "mz",
            "tz",
            "ug",
            "rw",
            "bi",
            "et",
            "er",
            "dj",
            "so",
            "sd",
            "ss",
            "td",
            "cf",
            "cm",
            "gq",
            "ga",
            "cg",
            "cd",
            "st",
            "gw",
            "gm",
            "sn",
            "mr",
            "ml",
            "bf",
            "ne",
            "ci",
            "tg",
            "bj",
            "lr",
            "sl",
            "cv",
            "ag",
            "dm",
            "gi",
            "gl",
            "im",
            "je",
            "ly",
            "pn",
            "vc",
            "vg",
        }

        # ç¡®ä¿åŸºç¡€ç›®å½•å­˜åœ¨
        os.makedirs(self.original_base_dir, exist_ok=True)
        os.makedirs(self.new_base_dir, exist_ok=True)

    def detect_rule_type(self, url):
        """ä»URLä¸­æ£€æµ‹è§„åˆ™ç±»å‹"""
        url_lower = url.lower()

        for keyword, directory in self.rule_type_mapping.items():
            pattern = f"/{keyword}/"
            if pattern in url_lower:
                return directory

        return "unknown"

    def get_directories_for_rule_type(self, rule_type):
        """æ ¹æ®è§„åˆ™ç±»å‹è·å–å¯¹åº”çš„åŸå§‹å’Œè¾“å‡ºç›®å½•"""
        original_dir = os.path.join(self.original_base_dir, rule_type)
        new_dir = os.path.join(self.new_base_dir, rule_type)

        os.makedirs(original_dir, exist_ok=True)
        os.makedirs(new_dir, exist_ok=True)

        return original_dir, new_dir

    def download_rule_list(self, url):
        """ä¸‹è½½è§„åˆ™åˆ—è¡¨åˆ°å¯¹åº”çš„originalå­æ–‡ä»¶å¤¹"""
        try:
            response = requests.get(url, timeout=30)
            response.raise_for_status()

            rule_type = self.detect_rule_type(url)
            print(f"  ğŸ” æ£€æµ‹åˆ°è§„åˆ™ç±»å‹: {rule_type}")

            original_dir, _ = self.get_directories_for_rule_type(rule_type)

            filename = os.path.basename(urlparse(url).path)
            if not filename:
                filename = "rules.list"

            filepath = os.path.join(original_dir, filename)

            with open(filepath, "w", encoding="utf-8") as f:
                f.write(response.text)

            print(f"  âœ“ å·²ä¸‹è½½: {filename}")
            print(f"  ğŸ“ ä¿å­˜è·¯å¾„: {os.path.relpath(filepath)}")
            return filepath, filename, rule_type

        except Exception as e:
            print(f"  âœ— ä¸‹è½½å¤±è´¥: {e}")
            return None, None, None

    def parse_header(self, lines):
        """è§£ææ–‡ä»¶å¤´éƒ¨ä¿¡æ¯"""
        header_lines = []
        header_info = {}
        content_start = 0

        for idx, line in enumerate(lines):
            stripped = line.strip()
            if stripped.startswith("#"):
                header_lines.append(line)

                if ":" in stripped:
                    parts = stripped[1:].split(":", 1)
                    if len(parts) == 2:
                        key = parts[0].strip()
                        value = parts[1].strip()
                        header_info[key] = value
            else:
                content_start = idx
                break

        return header_lines, header_info, content_start

    def count_rule_types(self, lines, start_index=0):
        """ç»Ÿè®¡å„ç±»è§„åˆ™çš„æ•°é‡"""
        counts = defaultdict(int)

        for line in lines[start_index:]:
            stripped = line.strip()
            if not stripped or stripped.startswith("#"):
                continue

            if stripped.startswith("DOMAIN-SUFFIX,"):
                counts["DOMAIN-SUFFIX"] += 1
            elif stripped.startswith("DOMAIN-KEYWORD,"):
                counts["DOMAIN-KEYWORD"] += 1
            elif stripped.startswith("DOMAIN,"):
                counts["DOMAIN"] += 1
            elif stripped.startswith("IP-CIDR6,"):
                counts["IP-CIDR6"] += 1
            elif stripped.startswith("IP-CIDR,"):
                counts["IP-CIDR"] += 1
            elif stripped.startswith("IP6-CIDR,"):
                counts["IP-CIDR6"] += 1
            elif stripped.startswith("USER-AGENT,"):
                counts["USER-AGENT"] += 1
            elif stripped.startswith("URL-REGEX,"):
                counts["URL-REGEX"] += 1
            elif stripped.startswith("PROCESS-NAME,"):
                counts["PROCESS-NAME"] += 1

        return counts

    def generate_header(self, header_info, rule_counts):
        """ç”Ÿæˆæ›´æ–°åçš„æ–‡ä»¶å¤´éƒ¨"""
        header_lines = []

        if "NAME" in header_info:
            header_lines.append(f"# NAME: {header_info['NAME']}\n")

        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        header_lines.append(f"# UPDATED: {current_time}\n")

        total = 0
        for rule_type in sorted(rule_counts.keys()):
            count = rule_counts[rule_type]
            total += count
            header_lines.append(f"# {rule_type}: {count}\n")

        header_lines.append(f"# TOTAL: {total}\n")

        return header_lines

    def extract_domain_from_line(self, line):
        """ä»è§„åˆ™è¡Œä¸­æå–åŸŸåå’Œè§„åˆ™ç±»å‹"""
        line = line.strip()

        if not line or line.startswith("#"):
            return None, None

        patterns = [
            (r"DOMAIN-SUFFIX,([^,\s]+)", "DOMAIN-SUFFIX"),
            (r"DOMAIN,([^,\s]+)", "DOMAIN"),
            (r"DOMAIN-KEYWORD,([^,\s]+)", "DOMAIN-KEYWORD"),
        ]

        for pattern, rule_type in patterns:
            match = re.search(pattern, line)
            if match:
                return match.group(1).lower(), rule_type

        return None, None

    def get_base_domain(self, domain):
        """
        æå–åŸºç¡€åŸŸåï¼ˆä¸»åŸŸåï¼‰

        æ­£ç¡®å¤„ç†å„ç§åŸŸåæ ¼å¼ï¼š
        1. äºŒçº§å›½å®¶åŸŸåï¼šgoogle.com.ag â†’ google
        2. äºŒçº§å›½å®¶åŸŸåï¼šgoogle.co.uk â†’ google
        3. å›½å®¶é¡¶çº§åŸŸåï¼šgoogle.cn â†’ google
        4. é€šç”¨é¡¶çº§åŸŸåï¼šgoogle.com â†’ google
        5. ç‰¹æ®Šç”¨é€”ccTLDï¼šgoogle.io â†’ google ï¼ˆä¸è§†ä¸ºåŒºåŸŸå˜ä½“ï¼‰
        6. æ–°é€šç”¨TLDï¼šgoogle.dev â†’ google
        """
        if not domain:
            return None

        # ç§»é™¤ www. å‰ç¼€
        domain = re.sub(r"^www\.", "", domain)

        # åˆ†å‰²åŸŸå
        parts = domain.split(".")

        if len(parts) < 2:
            return domain

        # å¤„ç†äºŒçº§å›½å®¶åŸŸåï¼ˆå¦‚ google.com.ag, google.co.ukï¼‰
        if len(parts) >= 3:
            tld = parts[-1]
            sld = parts[-2]

            # å¦‚æœæ˜¯"äºŒçº§æ ‡è¯†.å›½å®¶ä»£ç "çš„ç»„åˆï¼Œè¿”å›ä¸»åŸŸå
            if tld in self.regional_tlds and sld in self.second_level_indicators:
                return parts[-3] if len(parts) >= 3 else parts[0]

        # å¤„ç†æ™®é€šå›½å®¶é¡¶çº§åŸŸåï¼ˆå¦‚ google.cnï¼‰
        if parts[-1] in self.regional_tlds:
            return parts[-2] if len(parts) >= 2 else domain

        # å¤„ç†ç‰¹æ®Šç”¨é€”ccTLDï¼ˆå¦‚ google.io, google.aiï¼‰
        if parts[-1] in self.special_purpose_cctlds:
            return parts[-2] if len(parts) >= 2 else domain

        # å¤„ç†é€šç”¨é¡¶çº§åŸŸåï¼ˆå¦‚ google.com, google.devï¼‰
        if parts[-1] in self.generic_tlds:
            return parts[-2] if len(parts) >= 2 else domain

        # å…¶ä»–æƒ…å†µï¼Œè¿”å›å€’æ•°ç¬¬äºŒä¸ªéƒ¨åˆ†
        return parts[-2] if len(parts) >= 2 else domain

    def is_regional_variant(self, domain):
        """
        åˆ¤æ–­åŸŸåæ˜¯å¦æ˜¯åŒºåŸŸæ€§å˜ä½“

        åŒºåŸŸå˜ä½“åŒ…æ‹¬ï¼š
        1. äºŒçº§å›½å®¶åŸŸåï¼šyoutube.com.co, google.co.uk ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        2. å›½å®¶é¡¶çº§åŸŸåï¼šgoogle.cn, google.jp

        ä¸åŒ…æ‹¬ï¼š
        1. ç‰¹æ®Šç”¨é€”ccTLDï¼šgoogle.co, github.io, discord.gg
        2. é€šç”¨TLDï¼šgoogle.com, google.org
        3. æ–°é€šç”¨TLDï¼šgoogle.dev, google.ventures
        """
        if not domain:
            return False

        parts = domain.split(".")
        if len(parts) < 2:
            return False

        tld = parts[-1]

        # ã€å…³é”®ä¿®å¤ã€‘ä¼˜å…ˆæ£€æŸ¥äºŒçº§å›½å®¶åŸŸåç»“æ„
        # ä¾‹å¦‚ï¼š.com.co, .co.uk, .com.ag
        if len(parts) >= 3:
            sld = parts[-2]
            # å¦‚æœæ˜¯"äºŒçº§æ ‡è¯†.å›½å®¶ä»£ç "çš„ç»„åˆ
            # å³ä½¿TLDåœ¨special_purpose_cctldsä¸­ï¼Œä¹Ÿè§†ä¸ºåŒºåŸŸå˜ä½“
            if sld in self.second_level_indicators and tld in self.regional_tlds:
                return True

        # æ£€æŸ¥æ˜¯å¦æ˜¯ç‰¹æ®Šç”¨é€”ccTLDï¼ˆä»…å¯¹éäºŒçº§åŸŸåç”Ÿæ•ˆï¼‰
        if tld in self.special_purpose_cctlds:
            return False

        # æ£€æŸ¥æ˜¯å¦æ˜¯çº¯é€šç”¨TLD
        if tld in self.generic_tlds:
            return False

        # æ£€æŸ¥TLDæ˜¯å¦æ˜¯çœŸæ­£çš„åŒºåŸŸæ€§å›½å®¶ä»£ç 
        if tld in self.regional_tlds:
            return True

        return False

    def filter_rules(self, input_file, output_file, threshold=5):
        """è¿‡æ»¤è§„åˆ™æ–‡ä»¶ï¼Œç§»é™¤åŒºåŸŸæ€§åŸŸåå˜ä½“"""
        try:
            with open(input_file, "r", encoding="utf-8") as f:
                lines = f.readlines()

            header_lines, header_info, content_start = self.parse_header(lines)

            # ç¬¬ä¸€éï¼šåˆ†æåŸŸå
            base_domain_variants = defaultdict(list)
            domain_info = {}

            for idx in range(content_start, len(lines)):
                line = lines[idx]
                domain, rule_type = self.extract_domain_from_line(line)
                if domain:
                    base_domain = self.get_base_domain(domain)
                    is_regional = self.is_regional_variant(domain)

                    domain_info[idx] = (domain, rule_type, is_regional)

                    if is_regional and base_domain:
                        base_domain_variants[base_domain].append((domain, idx))

            # æ‰¾å‡ºéœ€è¦è¿‡æ»¤çš„åŸºç¡€åŸŸå
            base_domains_to_filter = set()
            for base_domain, variants in base_domain_variants.items():
                if len(variants) >= threshold:
                    base_domains_to_filter.add(base_domain)

            print(f"\n  ğŸ“‹ åŸŸåå˜ä½“åˆ†æ:")
            for base_domain in sorted(base_domain_variants.keys()):
                count = len(base_domain_variants[base_domain])
                if base_domain in base_domains_to_filter:
                    print(f"    ğŸ—‘ï¸  {base_domain}: {count} ä¸ªåŒºåŸŸå˜ä½“ â†’ å°†å‰”é™¤")
                else:
                    print(f"    âœ… {base_domain}: {count} ä¸ªåŒºåŸŸå˜ä½“ â†’ ä¿ç•™")

                # æ˜¾ç¤ºéƒ¨åˆ†åŸŸåç¤ºä¾‹
                display_count = min(10, count)
                for i, (domain, _) in enumerate(
                    base_domain_variants[base_domain][:display_count]
                ):
                    symbol = "    â”œâ”€" if i < display_count - 1 else "    â””â”€"
                    print(f"{symbol} {domain}")
                if count > display_count:
                    print(f"       ... è¿˜æœ‰ {count - display_count} ä¸ª")

            # ç¬¬äºŒéï¼šè¿‡æ»¤è§„åˆ™
            filtered_lines = []
            removed_count = 0
            removed_domains = []

            for idx, line in enumerate(lines):
                if idx < content_start:
                    continue

                if idx not in domain_info:
                    filtered_lines.append(line)
                    continue

                domain, rule_type, is_regional = domain_info[idx]
                base_domain = self.get_base_domain(domain)

                if is_regional and base_domain in base_domains_to_filter:
                    removed_count += 1
                    removed_domains.append(domain)
                    continue

                filtered_lines.append(line)

            rule_counts = self.count_rule_types(filtered_lines)
            new_header = self.generate_header(header_info, rule_counts)

            with open(output_file, "w", encoding="utf-8") as f:
                f.writelines(new_header)
                f.writelines(filtered_lines)

            print(f"\n  ğŸ“Š å¤„ç†ç»Ÿè®¡:")
            print(f"    â€¢ åŸå§‹è§„åˆ™æ•°: {len(lines) - content_start}")
            print(f"    â€¢ è¿‡æ»¤åè§„åˆ™: {len(filtered_lines)}")
            print(f"    â€¢ å·²ç§»é™¤è§„åˆ™: {removed_count}")
            if len(lines) - content_start > 0:
                print(
                    f"    â€¢ ä¿ç•™æ¯”ä¾‹: {len(filtered_lines) / (len(lines) - content_start) * 100:.1f}%"
                )

            if removed_domains and len(removed_domains) <= 30:
                print(f"\n  ğŸ—‘ï¸  ç§»é™¤çš„åŸŸåç¤ºä¾‹ (å‰ {min(len(removed_domains), 30)} ä¸ª):")
                for i, domain in enumerate(removed_domains[:30], 1):
                    print(f"    {i:2d}. {domain}")
                if len(removed_domains) > 30:
                    print(f"    ... è¿˜æœ‰ {len(removed_domains) - 30} ä¸ªåŸŸåè¢«ç§»é™¤")

            return True

        except Exception as e:
            print(f"  âŒ å¤„ç†å¤±è´¥: {e}")
            import traceback

            traceback.print_exc()
            return False

    def process_url(self, url, threshold=5):
        """å¤„ç†å•ä¸ªURL"""
        print(f"\n{'=' * 70}")
        print(f"ğŸš€ å¼€å§‹å¤„ç†è§„åˆ™åˆ—è¡¨")
        print(f"{'=' * 70}")
        print(f"ğŸ“ URL: {url}")
        print(f"ğŸ¯ é˜ˆå€¼: {threshold} (åŒºåŸŸå˜ä½“æ•°)")

        input_file, filename, rule_type = self.download_rule_list(url)
        if not input_file:
            return False

        _, new_dir = self.get_directories_for_rule_type(rule_type)
        output_file = os.path.join(new_dir, filename)

        success = self.filter_rules(input_file, output_file, threshold)

        if success:
            print(f"\nâœ… å¤„ç†å®Œæˆ!")
            print(f"ğŸ“ åŸå§‹æ–‡ä»¶: {os.path.relpath(input_file)}")
            print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {os.path.relpath(output_file)}")
            print(f"ğŸ·ï¸  è§„åˆ™ç±»å‹: {rule_type}")
        else:
            print(f"\nâŒ å¤„ç†å¤±è´¥!")

        return success

    def process_urls(self, urls, threshold=5):
        """æ‰¹é‡å¤„ç†å¤šä¸ªURL"""
        print(f"\n{'=' * 70}")
        print(f"ğŸš€ æ‰¹é‡å¤„ç†æ¨¡å¼")
        print(f"{'=' * 70}")
        print(f"ğŸ“¦ å¾…å¤„ç†åˆ—è¡¨: {len(urls)} ä¸ª")
        print(f"ğŸ¯ é˜ˆå€¼: {threshold}")

        results = []
        for i, url in enumerate(urls, 1):
            print(f"\n[{i}/{len(urls)}] å¤„ç†ä¸­...")
            success = self.process_url(url, threshold)

            rule_type = self.detect_rule_type(url)
            filename = os.path.basename(urlparse(url).path)
            results.append((url, filename, rule_type, success))

        print(f"\n{'=' * 70}")
        print(f"ğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆæ±‡æ€»")
        print(f"{'=' * 70}")
        success_count = sum(1 for _, _, _, success in results if success)
        fail_count = len(results) - success_count

        print(f"âœ… æˆåŠŸ: {success_count} ä¸ª")
        print(f"âŒ å¤±è´¥: {fail_count} ä¸ª")
        if len(results) > 0:
            print(f"ğŸ“ˆ æˆåŠŸç‡: {success_count / len(results) * 100:.1f}%")

        print(f"\nè¯¦ç»†ç»“æœ (æŒ‰è§„åˆ™ç±»å‹åˆ†ç»„):")
        by_type = defaultdict(list)
        for url, filename, rule_type, success in results:
            by_type[rule_type].append((filename, success))

        for rule_type in sorted(by_type.keys()):
            print(f"\n  ğŸ“‚ {rule_type.upper()}:")
            for filename, success in by_type[rule_type]:
                status = "âœ…" if success else "âŒ"
                print(f"    {status} {filename}")

        return results

    def list_directory_structure(self):
        """åˆ—å‡ºå½“å‰çš„ç›®å½•ç»“æ„"""
        print(f"\n{'=' * 70}")
        print(f"ğŸ“ å½“å‰ç›®å½•ç»“æ„")
        print(f"{'=' * 70}")

        for base_name, base_dir in [
            ("åŸå§‹æ–‡ä»¶", self.original_base_dir),
            ("å¤„ç†åæ–‡ä»¶", self.new_base_dir),
        ]:
            print(f"\n{base_name}: {os.path.relpath(base_dir)}")
            if os.path.exists(base_dir):
                subdirs = [
                    d
                    for d in os.listdir(base_dir)
                    if os.path.isdir(os.path.join(base_dir, d))
                ]
                if subdirs:
                    for subdir in sorted(subdirs):
                        subdir_path = os.path.join(base_dir, subdir)
                        file_count = len(
                            [
                                f
                                for f in os.listdir(subdir_path)
                                if os.path.isfile(os.path.join(subdir_path, f))
                            ]
                        )
                        print(f"  â”œâ”€ {subdir}/ ({file_count} ä¸ªæ–‡ä»¶)")
                else:
                    print(f"  â””â”€ (ç©º)")
            else:
                print(f"  â””â”€ (ç›®å½•ä¸å­˜åœ¨)")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("=" * 70)
    print("ğŸ”§ åŸŸåè§„åˆ™è¿‡æ»¤å·¥å…·")
    print("=" * 70)
    print("ğŸ“ åŠŸèƒ½: å‰”é™¤åŒºåŸŸæ€§åŸŸåå˜ä½“ï¼Œä¿ç•™é€šç”¨åŸŸå")
    print("ğŸ¯ ç‰¹æ€§: æ™ºèƒ½è¯†åˆ«è§„åˆ™ç±»å‹å¹¶åˆ†ç±»å­˜å‚¨")
    print("âœ¨ æ–°å¢: æ”¯æŒç‰¹æ®Šç”¨é€”ccTLDï¼ˆ.io, .ai, .ggç­‰ï¼‰")
    print("=" * 70)

    filter_tool = DomainRuleFilter()

    # æ˜¾ç¤ºç›®å½•ç»“æ„
    filter_tool.list_directory_structure()

    # æ‰¹é‡å¤„ç†URL
    urls = [
        # Clash/Mihomo è§„åˆ™
        "https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/refs/heads/master/rule/Clash/YouTube/YouTube.list",
        "https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/refs/heads/master/rule/Clash/Google/Google.list",
        "https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/refs/heads/master/rule/Clash/Facebook/Facebook.list",
        # Surge è§„åˆ™
        "https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/refs/heads/master/rule/Surge/YouTube/YouTube.list",
        "https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/refs/heads/master/rule/Surge/Google/Google.list",
        "https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/refs/heads/master/rule/Surge/Facebook/Facebook.list",
        # Loon è§„åˆ™
        "https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/refs/heads/master/rule/Loon/YouTube/YouTube.list",
        "https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/refs/heads/master/rule/Loon/Google/Google.list",
        "https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/refs/heads/master/rule/Loon/Facebook/Facebook.list",
    ]

    filter_tool.process_urls(urls, threshold=5)

    # å¤„ç†å®Œæˆåå†æ¬¡æ˜¾ç¤ºç›®å½•ç»“æ„
    filter_tool.list_directory_structure()

    print("\n" + "=" * 70)
    print("âœ¨ ç¨‹åºæ‰§è¡Œå®Œæ¯•")
    print("=" * 70)
